# -*- coding: utf-8 -*-
"""store-sales-time-series-forecasting-XGBoost_0.447-v4 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cRGWVTrx_aicrwWdvHofwxM-ZyWH9cU-
"""

import numpy as np 
import pandas as pd 
import gc

import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('train.csv')
stores=pd.read_csv('stores.csv')
transactions=pd.read_csv('transactions.csv')
test=pd.read_csv('test.csv')
holiday=pd.read_csv('holidays_events.csv')
submission=pd.read_csv('sample_submission.csv')
oil=pd.read_csv('oil.csv')

df.head()

df.tail()

"""**Train set is from 2013-01-01 to 2017-08-15**"""

stores.head()

transactions.head()

test.head()

submission.head()

holiday.head()

holiday[holiday['transferred']==True]

holiday.type.unique()

oil.head()

df_sls_store=df.merge(stores, on='store_nbr', how='left')
df_sls_str_tr=df_sls_store.merge(transactions, on=['store_nbr', 'date'],how='left')
df_sls_str_tr_oil=df_sls_str_tr.merge(oil, on='date', how='left')
train_1=df_sls_str_tr_oil.merge(holiday, on='date', how='left')
test=test.merge(stores, on='store_nbr', how='left')
test=test.merge(transactions, on=['store_nbr', 'date'],how='left')
test=test.merge(oil, on='date', how='left')
test=test.merge(holiday, on='date', how='left')

del df_sls_store, df_sls_str_tr, df_sls_str_tr_oil
gc.collect()

train_1.info()

train_1.head()

train_1=train_1.rename({'type_x':'store_type', 'type_y':'holiday_type'}, axis=1)

test=test.rename({'type_x':'store_type', 'type_y':'holiday_type'}, axis=1)

train_1=train_1.drop(['locale_name','description'], axis=1)
train_1['date']=pd.to_datetime(train_1['date'])

test=test.drop(['locale_name','description'], axis=1)
test['date']=pd.to_datetime(test['date'])

train_1['holiday_type']=train_1['holiday_type'].fillna('None')
test['holiday_type']=test['holiday_type'].fillna('None')

def split_date(df):
    df['Year'] = df.date.dt.year
    df['Month'] = df.date.dt.month
    df['Day'] = df.date.dt.day
    df['WeekOfYear'] = (df.date.dt.isocalendar().week)*1.0 

split_date(train_1)
split_date(test)

train_1.rename({'dcoilwtico':'Oil_Price'}, inplace=True, axis=1)
test.rename({'dcoilwtico':'Oil_Price'}, inplace=True, axis=1)

train_1.head()

"""#  Exploratory Data Analysis

## Average Sales per Store Type
"""

AvgSale=train_1.groupby('store_type')['sales'].mean().to_dict()
df = pd.DataFrame(list(AvgSale.items()), columns=['Store_Type', 'AvgSales']).sort_values(by='AvgSales', ascending=False)

plt.bar('Store_Type','AvgSales',data=df);

del df, AvgSale
gc.collect()

"""## Average Sales per Family"""

family_AvgSale=train_1.groupby('family')['sales'].mean().to_dict()
df_2 = pd.DataFrame(list(family_AvgSale.items()), columns=['family', 'AvgSales']).sort_values(by='AvgSales', ascending=False)
plt.figure(figsize=(14,5))
plt.bar('family', 'AvgSales', data=df_2)
plt.title('Average Sales per family')
plt.xlabel('Family')
plt.ylabel('Avg_Sales')
plt.xticks(rotation=90);

del family_AvgSale, df_2
gc.collect()

"""## Average Yearly Sales"""

Yearly_AvgSales=train_1.groupby('Year')['sales'].mean().to_dict()
df_3 = pd.DataFrame(list(Yearly_AvgSales.items()), columns=['Year', 'AvgSales']).sort_values(by='AvgSales', ascending=False)
plt.figure(figsize=(14,5))
plt.bar('Year', 'AvgSales', data=df_3)
plt.title('Average Sales per Year')
plt.xlabel('Year')
plt.ylabel('Avg_Sales')
plt.xticks(rotation=90);

del Yearly_AvgSales, df_3
gc.collect()

"""## Average Weekly Sales by Year"""

weekly_sales_2013=train_1[train_1.Year==2013].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2014=train_1[train_1.Year==2014].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2015=train_1[train_1.Year==2015].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2016=train_1[train_1.Year==2016].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2017=train_1[train_1.Year==2017].groupby('WeekOfYear')['sales'].mean()

plt.figure(figsize=(14,5))
plt.plot(weekly_sales_2013.index, weekly_sales_2013.values, label='2013')
plt.plot(weekly_sales_2014.index, weekly_sales_2014.values, label='2014')
plt.plot(weekly_sales_2015.index, weekly_sales_2015.values,label='2015')
plt.plot(weekly_sales_2016.index, weekly_sales_2016.values, label='2016')
plt.plot(weekly_sales_2017.index, weekly_sales_2017.values,label='2017')

plt.xticks(np.arange(1, 53, step=1), fontsize=10, rotation=90)
plt.yticks( fontsize=16)
plt.xlabel('Week of Year', fontsize=20, labelpad=20)
plt.ylabel('Sales', fontsize=20, labelpad=20)
plt.legend(bbox_to_anchor=(1.09,1));

del weekly_sales_2013,weekly_sales_2014, weekly_sales_2015,weekly_sales_2016,weekly_sales_2017
gc.collect()

"""## Oil Price vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.Oil_Price, train_1.sales, hue=train_1.store_type)

"""## City vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.city, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## Holiday Type vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.holiday_type, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## Cluster vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.cluster, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## locale vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.locale, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## Correlation Matrix"""

sns.set(style="white")
corr = train_1.corr()
mask = np.triu(np.ones_like(corr, dtype=np.bool))
f, ax = plt.subplots(figsize=(15, 10))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
plt.title('Correlation Matrix', fontsize=18)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
plt.show()

storetype_values = {'A':4, 'B':2, 'C':0,'D':3,'E':1}
train_1['store_type_numeric'] = train_1['store_type'].map(storetype_values)
test['store_type_numeric'] = test['store_type'].map(storetype_values)

print("Ratio of NaN in Training Set's Transactions column is:{:.2f}%".format(train_1.transactions.isna().sum()/len(train_1.transactions)*100))
print("Ratio of NaN in Test Set's Transactions column is:{:.2f}%".format(test.transactions.isna().sum()/len(test.transactions)*100))

train_1.drop('transactions',axis=1,inplace=True)
test.drop('transactions', axis=1, inplace=True)

train_1.drop(['locale','state','Oil_Price','transferred'],axis=1,inplace=True)
test.drop(['locale','state','Oil_Price','transferred'], axis=1, inplace=True)

train_1.head()

train_2=train_1.copy()
train_2['IsHoliday']=train_2['holiday_type'].apply(lambda x: 0 if x=='None' or x=='Work Day' else 1)

test['IsHoliday']=test['holiday_type'].apply(lambda x: 0 if x=='None' or x=='Work Day' else 1)

gc.collect()

train_2.drop(['store_type', 'holiday_type'], axis=1, inplace=True)
test.drop(['store_type', 'holiday_type'], axis=1, inplace=True)

del train_1
gc.collect()

! pip install --upgrade category_encoders

"""### To reflect Extra Sales in 2016 because of EarthQuake"""

train_3=train_2.copy()
train_3.loc[(train_3['Year']==2016) & (train_3['WeekOfYear']==16), 'IsHoliday']=1

"""# Feature Engineering"""

train_3.head()

"""## Avg Sales per Features"""

train_4=train_3.copy()
train_4['sales_log1p']=np.log1p(train_4['sales'])

train_4.drop('sales', axis=1, inplace=True)

train_4['Avg_store_family']=train_4.groupby(['store_nbr','family'])['sales_log1p'].transform('mean')

train_4.tail()

test['Avg_store_family']=train_4.groupby(['store_nbr','family'])['sales_log1p'].transform('mean')

train_4['Avg_store_family_day_mean']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('mean')
train_4['Avg_store_family_day_median']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('median')
train_4['weekend'] = train_4['date'].dt.weekday

test['Avg_store_family_day_mean']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('mean')
test['Avg_store_family_day_median']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('median')
test['weekend'] = test['date'].dt.weekday

train_4.head()

test.head()

train_5=train_4.copy()
X_encode=train_5.sample(frac=0.20, random_state=0)
y_encode = X_encode.pop('sales_log1p')

X_pretrain = train_5.drop(X_encode.index)
y_train = X_pretrain.pop('sales_log1p')

from category_encoders import MEstimateEncoder
encoder = MEstimateEncoder(cols=['family','city'], m=5.0)
encoder.fit(X_encode, y_encode)
X_train = encoder.transform(X_pretrain)
X_test=encoder.transform(test)

gc.collect()

X_train.head()

y_train.head()

X_test.head()

X_train.isna().sum()

X_test.isna().sum()

X_train.drop(['id','date'], axis=1, inplace=True)
X_test.drop(['id','date'], axis=1, inplace=True)

"""## Scale Numeric Columns"""

from sklearn.preprocessing import MinMaxScaler

Scaler=MinMaxScaler().fit(X_train)
X_train=Scaler.transform(X_train)
X_test=Scaler.transform(X_test)

len(X_train)

len(y_train)

"""# Machine Learning _ XGBoost

## Finding Model Parameters

** Must reset y_train index after scaling X_train cause X_train index has been reset
"""

y_train.dropna(inplace=True)
y_train.reset_index(drop=True, inplace=True)

from sklearn.model_selection import train_test_split

train_X, test_X , train_y , test_y =train_test_split(X_train, y_train, test_size=0.1, random_state=42)

! pip install xgboost

import xgboost as xgb

from sklearn.metrics import mean_squared_error as mse
def RMSLE(y_pred, y_test):
    return np.sqrt(mse(y_test,y_pred))

def param_test(params):
  d_train=xgb.DMatrix(train_X, label=train_y)
  d_test=xgb.DMatrix(test_X, test_y)
  evals=[(d_train, 'train'), (d_test, 'test')]
  model=xgb.train(dtrain=d_train,evals=evals,params=params)
  rmsle_train=RMSLE(model.predict(xgb.DMatrix(train_X)), train_y)
  rmsle_test=RMSLE(model.predict(xgb.DMatrix(test_X)), test_y)
  print('train rmsle:{:.3f} , test rmsle:{:.3f}'.format(rmsle_train, rmsle_test))

"""### max_depth"""

param_test({'colsample_bytree':0.7})

param_test({'colsample_bytree':0.8})

param_test({'colsample_bytree':0.8,'max_depth':5 })

param_test({'colsample_bytree':0.8,'max_depth':10 })

param_test({'colsample_bytree':0.8,'max_depth':15 })

param_test({'colsample_bytree':0.8,'max_depth':15,'subsample':0.7 })

param_test({'colsample_bytree':0.8,'max_depth':15,'subsample':0.9 })

param_test({'colsample_bytree':0.8,'max_depth':15,'subsample':0.9,'eta':0.01 })

param_test({'colsample_bytree':0.8,'max_depth':15,'subsample':0.9,'eta':0.8 })

param_test({'colsample_bytree':0.8,'max_depth':15,'subsample':0.9,'eta':0.5 })

X_train=pd.DataFrame(X_train)
X_test=pd.DataFrame(X_test)

from sklearn.model_selection import KFold
params = {
         'max_depth':15,
         'eta':0.1, #learning rate
         'subsample':0.9,
         'colsample_bytree':0.8,
         'silent':1,
         'objective':'reg:linear',
         }
kf=KFold(n_splits=5)
num_round=1000
models=[]

for train_idx, test_idx in kf.split(X_train):
    kf_X_train=X_train.iloc[train_idx]
    kf_y_train=y_train.iloc[train_idx]
    
    kf_X_test=X_train.iloc[test_idx]
    kf_y_test=y_train.iloc[test_idx]

    dtrain = xgb.DMatrix(kf_X_train, kf_y_train)
    dtest = xgb.DMatrix(kf_X_test,  kf_y_test)

    model = xgb.train(params, dtrain, num_boost_round=num_round, evals=[(dtrain, 'train'), (dtest, 'test')], verbose_eval=25, early_stopping_rounds=50)

    models.append(model)

"""### **Prediction**"""

y_pred=[]
d_pred=xgb.DMatrix(X_test)
for model in models:
    if y_pred ==[]:
        y_pred=np.expm1(model.predict(d_pred)) / len(models)
    else:
        y_pred +=np.expm1(model.predict(d_pred)) / len(models)

y_pred

y_pred.min()

y_pred=np.clip(y_pred,0,None)

y_pred.min()

"""## *RMSLE Calculation*"""

y_train_pred=[]
d_pred_train=xgb.DMatrix(X_train)
for model in models:
    if y_train_pred ==[]:
        y_train_pred=model.predict(d_pred_train) / len(models)
    else:
        y_train_pred +=model.predict(d_pred_train) / len(models)

RMSLE(y_train_pred, y_train)

"""### **Preparing Submission File**"""

y_pred=y_pred.reshape(-1,1)

submission['pred']=y_pred

submission.head()

submission.drop('sales', axis=1, inplace=True)

submission=submission.rename({'pred':'sales'}, axis=1)

submission.to_csv('submission.csv', index=False, float_format='%.3f')
