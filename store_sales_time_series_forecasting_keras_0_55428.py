# -*- coding: utf-8 -*-
"""store-sales-time-series-forecasting-Keras_0.55428-v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YkN3RBBWkcN_X_BPVQjZkdq_ek8NFNcN
"""

import numpy as np 
import pandas as pd 
import gc

import matplotlib.pyplot as plt
import seaborn as sns

def reduce_mem_usage(df, verbose=True):
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage(deep=True).sum() / 1024 ** 2 # just added 
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)    
    end_mem = df.memory_usage(deep=True).sum() / 1024 ** 2
    percent = 100 * (start_mem - end_mem) / start_mem
    print('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_mem, end_mem, percent))
    return df

! pip install tensorflow

import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Embedding, Dense, Activation, Dropout, Flatten
import keras

df=pd.read_csv('train.csv')
stores=pd.read_csv('stores.csv')
transactions=pd.read_csv('transactions.csv')
test=pd.read_csv('test.csv')
holiday=pd.read_csv('holidays_events.csv')
submission=pd.read_csv('sample_submission.csv')
oil=pd.read_csv('oil.csv')

df.head()

df.tail()

"""**Train set is from 2013-01-01 to 2017-08-15**"""

stores.head()

transactions.head()

test.head()

submission.head()

holiday.head()

holiday[holiday['transferred']==True]

holiday.type.unique()

oil.head()

df_sls_store=df.merge(stores, on='store_nbr', how='left')
df_sls_str_tr=df_sls_store.merge(transactions, on=['store_nbr', 'date'],how='left')
df_sls_str_tr_oil=df_sls_str_tr.merge(oil, on='date', how='left')
train_1=df_sls_str_tr_oil.merge(holiday, on='date', how='left')
test=test.merge(stores, on='store_nbr', how='left')
test=test.merge(transactions, on=['store_nbr', 'date'],how='left')
test=test.merge(oil, on='date', how='left')
test=test.merge(holiday, on='date', how='left')

del df_sls_store, df_sls_str_tr, df_sls_str_tr_oil
gc.collect()

train_1.info()

train_1.head()

train_1=train_1.rename({'type_x':'store_type', 'type_y':'holiday_type'}, axis=1)

test=test.rename({'type_x':'store_type', 'type_y':'holiday_type'}, axis=1)

train_1=train_1.drop(['locale_name','description'], axis=1)
train_1['date']=pd.to_datetime(train_1['date'])

test=test.drop(['locale_name','description'], axis=1)
test['date']=pd.to_datetime(test['date'])

train_1['holiday_type']=train_1['holiday_type'].fillna('None')
test['holiday_type']=test['holiday_type'].fillna('None')

def split_date(df):
    df['Year'] = df.date.dt.year
    df['Month'] = df.date.dt.month
    df['Day'] = df.date.dt.day
    df['WeekOfYear'] = (df.date.dt.isocalendar().week)*1.0 

split_date(train_1)
split_date(test)

train_1.rename({'dcoilwtico':'Oil_Price'}, inplace=True, axis=1)
test.rename({'dcoilwtico':'Oil_Price'}, inplace=True, axis=1)

train_1.head()

"""#  Exploratory Data Analysis

## Average Sales per Store Type
"""

AvgSale=train_1.groupby('store_type')['sales'].mean().to_dict()
df = pd.DataFrame(list(AvgSale.items()), columns=['Store_Type', 'AvgSales']).sort_values(by='AvgSales', ascending=False)

plt.bar('Store_Type','AvgSales',data=df);

del df, AvgSale
gc.collect()

"""## Average Sales per Family"""

family_AvgSale=train_1.groupby('family')['sales'].mean().to_dict()
df_2 = pd.DataFrame(list(family_AvgSale.items()), columns=['family', 'AvgSales']).sort_values(by='AvgSales', ascending=False)
plt.figure(figsize=(14,5))
plt.bar('family', 'AvgSales', data=df_2)
plt.title('Average Sales per family')
plt.xlabel('Family')
plt.ylabel('Avg_Sales')
plt.xticks(rotation=90);

del family_AvgSale, df_2
gc.collect()

"""## Average Yearly Sales"""

Yearly_AvgSales=train_1.groupby('Year')['sales'].mean().to_dict()
df_3 = pd.DataFrame(list(Yearly_AvgSales.items()), columns=['Year', 'AvgSales']).sort_values(by='AvgSales', ascending=False)
plt.figure(figsize=(14,5))
plt.bar('Year', 'AvgSales', data=df_3)
plt.title('Average Sales per Year')
plt.xlabel('Year')
plt.ylabel('Avg_Sales')
plt.xticks(rotation=90);

del Yearly_AvgSales, df_3
gc.collect()

"""## Average Weekly Sales by Year"""

weekly_sales_2013=train_1[train_1.Year==2013].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2014=train_1[train_1.Year==2014].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2015=train_1[train_1.Year==2015].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2016=train_1[train_1.Year==2016].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2017=train_1[train_1.Year==2017].groupby('WeekOfYear')['sales'].mean()

plt.figure(figsize=(14,5))
plt.plot(weekly_sales_2013.index, weekly_sales_2013.values, label='2013')
plt.plot(weekly_sales_2014.index, weekly_sales_2014.values, label='2014')
plt.plot(weekly_sales_2015.index, weekly_sales_2015.values,label='2015')
plt.plot(weekly_sales_2016.index, weekly_sales_2016.values, label='2016')
plt.plot(weekly_sales_2017.index, weekly_sales_2017.values,label='2017')

plt.xticks(np.arange(1, 53, step=1), fontsize=10, rotation=90)
plt.yticks( fontsize=16)
plt.xlabel('Week of Year', fontsize=20, labelpad=20)
plt.ylabel('Sales', fontsize=20, labelpad=20)
plt.legend(bbox_to_anchor=(1.09,1));

del weekly_sales_2013,weekly_sales_2014, weekly_sales_2015,weekly_sales_2016,weekly_sales_2017
gc.collect()

"""## Oil Price vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.Oil_Price, train_1.sales, hue=train_1.store_type)

"""## City vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.city, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## Holiday Type vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.holiday_type, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## Cluster vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.cluster, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## locale vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.locale, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## Correlation Matrix"""

sns.set(style="white")
corr = train_1.corr()
mask = np.triu(np.ones_like(corr, dtype=np.bool))
f, ax = plt.subplots(figsize=(15, 10))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
plt.title('Correlation Matrix', fontsize=18)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
plt.show()

storetype_values = {'A':4, 'B':2, 'C':0,'D':3,'E':1}
train_1['store_type_numeric'] = train_1['store_type'].map(storetype_values)
test['store_type_numeric'] = test['store_type'].map(storetype_values)

print("Ratio of NaN in Training Set's Transactions column is:{:.2f}%".format(train_1.transactions.isna().sum()/len(train_1.transactions)*100))
print("Ratio of NaN in Test Set's Transactions column is:{:.2f}%".format(test.transactions.isna().sum()/len(test.transactions)*100))

train_1.drop('transactions',axis=1,inplace=True)
test.drop('transactions', axis=1, inplace=True)

train_1.drop(['locale','Oil_Price','transferred'],axis=1,inplace=True)
test.drop(['locale','Oil_Price','transferred'], axis=1, inplace=True)

train_1.head()

train_2=train_1.copy()
train_2['IsHoliday']=train_2['holiday_type'].apply(lambda x: 0 if x=='None' or x=='Work Day' else 1)

test['IsHoliday']=test['holiday_type'].apply(lambda x: 0 if x=='None' or x=='Work Day' else 1)

gc.collect()

train_2.drop(['store_type', 'holiday_type'], axis=1, inplace=True)
test.drop(['store_type', 'holiday_type'], axis=1, inplace=True)

del train_1
gc.collect()

! pip install --upgrade category_encoders

"""### To reflect Extra Sales in 2016 because of EarthQuake"""

train_3=train_2.copy()
train_3.loc[(train_3['Year']==2016) & (train_3['WeekOfYear']==16), 'IsHoliday']=1

"""# Feature Engineering"""

train_3.head()

"""## Avg Sales per Features"""

train_4=train_3.copy()
train_4['sales_log1p']=np.log1p(train_4['sales'])

train_4.drop('sales', axis=1, inplace=True)

train_4['Avg_store_family']=train_4.groupby(['store_nbr','family'])['sales_log1p'].transform('mean')

test['Avg_store_family']=train_4.groupby(['store_nbr','family'])['sales_log1p'].transform('mean')

train_4['Avg_store_family_day_mean']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('mean')
train_4['Avg_store_family_day_median']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('median')
train_4['weekend'] = train_4['date'].dt.weekday

test['Avg_store_family_day_mean']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('mean')
test['Avg_store_family_day_median']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('median')
test['weekend'] = test['date'].dt.weekday

train_5=train_4.copy()
X_encode=train_5.sample(frac=0.20, random_state=0)
y_encode = X_encode.pop('sales_log1p')

X_pretrain = train_5.drop(X_encode.index)
y_train = X_pretrain.pop('sales_log1p')

from category_encoders import MEstimateEncoder
encoder = MEstimateEncoder(cols=['family','city','state'], m=5.0)
encoder.fit(X_encode, y_encode)
X_train = encoder.transform(X_pretrain)
X_test=encoder.transform(test)

gc.collect()

del train_5
gc.collect()

X_train.drop(['id','date'], axis=1, inplace=True)
X_test.drop(['id','date'], axis=1, inplace=True)

for df in [X_train, X_test]:
  reduce_mem_usage(df)

"""## Scale Numeric Columns"""

from sklearn.preprocessing import MinMaxScaler

Scaler=MinMaxScaler().fit(X_train)
X_train=Scaler.transform(X_train)
X_test=Scaler.transform(X_test)

len(X_train)

len(y_train)

"""# Machine Learning _ Keras

** Must reset y_train index after scaling X_train cause X_train index has been reset
"""

y_train.dropna(inplace=True)
y_train.reset_index(drop=True, inplace=True)

#from sklearn.model_selection import train_test_split

#train_X, test_X , train_y , test_y =train_test_split(X_train, y_train, test_size=0.1, random_state=42)

"""### **Creating the Model**"""

X_train.shape

"""### **Defining Layers**"""

dropout=0.5
in_sz=X_train.shape[1]

input=Input(in_sz,dtype='float32')
Layer=Dense(200, activation='relu')(input)
Layer= BatchNormalization()(Layer)
Layer=Dropout(dropout)(Layer)
Layer= Dense(150, activation='relu')(Layer)
Layer= BatchNormalization()(Layer)
Layer= Dropout(dropout)(Layer)
Layer= Dense(100, activation='relu')(Layer)
Layer= BatchNormalization()(Layer)
Layer= Dropout(dropout)(Layer)
Layer= Dense(100, activation='relu')(Layer)
Layer= BatchNormalization()(Layer)
Layer= Dropout(dropout)(Layer)
Layer= Dense(1, activation='linear')(Layer)

"""### **Defining Model**"""

model=Model(input, Layer)
model.compile(loss='mean_squared_error', optimizer='adam')

"""### **Fit the Model**"""

from sklearn.model_selection import KFold
from keras.callbacks import EarlyStopping

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=10)

epochs=50
n_splits=5
kf=KFold(n_splits=n_splits)
y_pred=np.zeros((X_test.shape[0],1))
y_train_pred=np.zeros((X_train.shape[0],1))

for i, (tr_idx, te_idx) in enumerate(kf.split(X_train)):
  print('Training Fold:', i+1,'\n')
  train_X=X_train[tr_idx]
  train_y=y_train[tr_idx]
  test_X=X_train[te_idx]
  test_y=y_train[te_idx]

  model.fit(X_train,y_train,batch_size=1200, epochs=epochs,validation_data=[test_X, test_y], callbacks=[es])

  y_train_pred+=(model.predict(X_train))/n_splits
  y_pred+=(np.expm1(model.predict(X_test)))/n_splits

"""### **Prediction**"""

y_pred

y_pred.min()

#y_pred=np.clip(y_pred,0,None)

#y_pred.min()

np.save('y_pred_keras', y_pred)

"""## *RMSLE Calculation*"""

from sklearn.metrics import mean_squared_error as mse
def RMSLE(y_pred, y_test):
    return np.sqrt(mse(y_test,y_pred))

RMSLE(y_train_pred, y_train)

"""### **Preparing Submission File**"""

#y_pred=y_pred.reshape(-1,1)

submission['pred']=y_pred

submission.head()

submission.drop('sales', axis=1, inplace=True)

submission=submission.rename({'pred':'sales'}, axis=1)

submission.to_csv('submission.csv', index=False, float_format='%.3f')