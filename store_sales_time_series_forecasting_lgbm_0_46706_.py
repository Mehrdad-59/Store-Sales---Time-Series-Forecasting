# -*- coding: utf-8 -*-
"""store-sales-time-series-forecasting-lgbm_0.46706-v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FZnkTtoz78tMNULxsTrVPb4g8qDOW-NK
"""

import numpy as np
import pandas as pd 
import gc

import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('train.csv')
stores=pd.read_csv('stores.csv')
transactions=pd.read_csv('transactions.csv')
test=pd.read_csv('test.csv')
holiday=pd.read_csv('holidays_events.csv')
submission=pd.read_csv('sample_submission.csv')
oil=pd.read_csv('oil.csv')

df.head()

df.tail()

"""**Train set is from 2013-01-01 to 2017-08-15**"""

stores.head()

transactions.head()

test.head()

submission.head()

holiday.head()

holiday[holiday['transferred']==True]

holiday.type.unique()

oil.head()

df_sls_store=df.merge(stores, on='store_nbr', how='left')
df_sls_str_tr=df_sls_store.merge(transactions, on=['store_nbr', 'date'],how='left')
df_sls_str_tr_oil=df_sls_str_tr.merge(oil, on='date', how='left')
train_1=df_sls_str_tr_oil.merge(holiday, on='date', how='left')
test=test.merge(stores, on='store_nbr', how='left')
test=test.merge(transactions, on=['store_nbr', 'date'],how='left')
test=test.merge(oil, on='date', how='left')
test=test.merge(holiday, on='date', how='left')

del df_sls_store, df_sls_str_tr, df_sls_str_tr_oil
gc.collect()

train_1.info()

train_1.head()

train_1=train_1.rename({'type_x':'store_type', 'type_y':'holiday_type'}, axis=1)

test=test.rename({'type_x':'store_type', 'type_y':'holiday_type'}, axis=1)

train_1=train_1.drop(['locale_name','description'], axis=1)
train_1['date']=pd.to_datetime(train_1['date'])

test=test.drop(['locale_name','description'], axis=1)
test['date']=pd.to_datetime(test['date'])

train_1['holiday_type']=train_1['holiday_type'].fillna('None')
test['holiday_type']=test['holiday_type'].fillna('None')

def split_date(df):
    df['Year'] = df.date.dt.year
    df['Month'] = df.date.dt.month
    df['Day'] = df.date.dt.day
    df['WeekOfYear'] = (df.date.dt.isocalendar().week)*1.0 

split_date(train_1)
split_date(test)

train_1.rename({'dcoilwtico':'Oil_Price'}, inplace=True, axis=1)
test.rename({'dcoilwtico':'Oil_Price'}, inplace=True, axis=1)

train_1.head()

"""#  Exploratory Data Analysis

## Average Sales per Store Type
"""

AvgSale=train_1.groupby('store_type')['sales'].mean().to_dict()
df = pd.DataFrame(list(AvgSale.items()), columns=['Store_Type', 'AvgSales']).sort_values(by='AvgSales', ascending=False)

plt.bar('Store_Type','AvgSales',data=df);

del df, AvgSale
gc.collect()

"""## Average Sales per Family"""

family_AvgSale=train_1.groupby('family')['sales'].mean().to_dict()
df_2 = pd.DataFrame(list(family_AvgSale.items()), columns=['family', 'AvgSales']).sort_values(by='AvgSales', ascending=False)
plt.figure(figsize=(14,5))
plt.bar('family', 'AvgSales', data=df_2)
plt.title('Average Sales per family')
plt.xlabel('Family')
plt.ylabel('Avg_Sales')
plt.xticks(rotation=90);

del family_AvgSale, df_2
gc.collect()

"""## Average Yearly Sales"""

Yearly_AvgSales=train_1.groupby('Year')['sales'].mean().to_dict()
df_3 = pd.DataFrame(list(Yearly_AvgSales.items()), columns=['Year', 'AvgSales']).sort_values(by='AvgSales', ascending=False)
plt.figure(figsize=(14,5))
plt.bar('Year', 'AvgSales', data=df_3)
plt.title('Average Sales per Year')
plt.xlabel('Year')
plt.ylabel('Avg_Sales')
plt.xticks(rotation=90);

del Yearly_AvgSales, df_3
gc.collect()

"""## Average Weekly Sales by Year"""

weekly_sales_2013=train_1[train_1.Year==2013].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2014=train_1[train_1.Year==2014].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2015=train_1[train_1.Year==2015].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2016=train_1[train_1.Year==2016].groupby('WeekOfYear')['sales'].mean()
weekly_sales_2017=train_1[train_1.Year==2017].groupby('WeekOfYear')['sales'].mean()

plt.figure(figsize=(14,5))
plt.plot(weekly_sales_2013.index, weekly_sales_2013.values, label='2013')
plt.plot(weekly_sales_2014.index, weekly_sales_2014.values, label='2014')
plt.plot(weekly_sales_2015.index, weekly_sales_2015.values,label='2015')
plt.plot(weekly_sales_2016.index, weekly_sales_2016.values, label='2016')
plt.plot(weekly_sales_2017.index, weekly_sales_2017.values,label='2017')

plt.xticks(np.arange(1, 53, step=1), fontsize=10, rotation=90)
plt.yticks( fontsize=16)
plt.xlabel('Week of Year', fontsize=20, labelpad=20)
plt.ylabel('Sales', fontsize=20, labelpad=20)
plt.legend(bbox_to_anchor=(1.09,1));

del weekly_sales_2013,weekly_sales_2014, weekly_sales_2015,weekly_sales_2016,weekly_sales_2017
gc.collect()

"""## Oil Price vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.Oil_Price, train_1.sales, hue=train_1.store_type)

"""## City vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.city, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## Holiday Type vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.holiday_type, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## Cluster vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.cluster, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## locale vs. Sales"""

plt.figure(figsize=(14,8))
sns.scatterplot(train_1.locale, train_1.sales, hue=train_1.store_type)
plt.xticks(rotation=90);

"""## Correlation Matrix"""

sns.set(style="white")
corr = train_1.corr()
mask = np.triu(np.ones_like(corr, dtype=np.bool))
f, ax = plt.subplots(figsize=(15, 10))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
plt.title('Correlation Matrix', fontsize=18)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
plt.show()

storetype_values = {'A':4, 'B':2, 'C':0,'D':3,'E':1}
train_1['store_type_numeric'] = train_1['store_type'].map(storetype_values)
test['store_type_numeric'] = test['store_type'].map(storetype_values)

print("Ratio of NaN in Training Set's Transactions column is:{:.2f}%".format(train_1.transactions.isna().sum()/len(train_1.transactions)*100))
print("Ratio of NaN in Test Set's Transactions column is:{:.2f}%".format(test.transactions.isna().sum()/len(test.transactions)*100))

train_1.drop('transactions',axis=1,inplace=True)
test.drop('transactions', axis=1, inplace=True)

train_1.drop(['locale','Oil_Price','transferred'],axis=1,inplace=True)
test.drop(['locale','Oil_Price','transferred'], axis=1, inplace=True)

train_1.head()

train_2=train_1.copy()
train_2['IsHoliday']=train_2['holiday_type'].apply(lambda x: 0 if x=='None' or x=='Work Day' else 1)

test['IsHoliday']=test['holiday_type'].apply(lambda x: 0 if x=='None' or x=='Work Day' else 1)

gc.collect()

train_2.drop(['store_type', 'holiday_type'], axis=1, inplace=True)
test.drop(['store_type', 'holiday_type'], axis=1, inplace=True)

del train_1
gc.collect()

! pip install --upgrade category_encoders

"""### To reflect Extra Sales in 2016 because of EarthQuake"""

train_3=train_2.copy()
train_3.loc[(train_3['Year']==2016) & (train_3['WeekOfYear']==16), 'IsHoliday']=1

"""# Feature Engineering"""

train_3.head()

"""## Avg Sales per Features"""

train_4=train_3.copy()
train_4['sales_log1p']=np.log1p(train_4['sales'])

train_4.drop('sales', axis=1, inplace=True)

train_4['Avg_store_family']=train_4.groupby(['store_nbr','family'])['sales_log1p'].transform('mean')

train_4.tail()

test['Avg_store_family']=train_4.groupby(['store_nbr','family'])['sales_log1p'].transform('mean')

train_4['Avg_store_family_day_mean']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('mean')
train_4['Avg_store_family_day_median']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('median')
train_4['weekend'] = train_4['date'].dt.weekday

test['Avg_store_family_day_mean']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('mean')
test['Avg_store_family_day_median']=train_4.groupby(['store_nbr','family','Day'])['sales_log1p'].transform('median')
test['weekend'] = test['date'].dt.weekday

train_4.columns

test.head()

for c in train_4.columns:
    col_type = train_4[c].dtype
    if col_type == 'object' or col_type.name == 'category':
        train_4[c] = train_4[c].astype('category')

cat_cols=[col for col in train_4.columns if train_4[col].dtype in ['object', 'category']]
cat_cols

train_5=train_4.copy()
X_encode=train_5.sample(frac=0.20, random_state=0)
y_encode = X_encode.pop('sales_log1p')

X_pretrain = train_5.drop(X_encode.index)
y_train = X_pretrain.pop('sales_log1p')

from category_encoders import MEstimateEncoder
encoder = MEstimateEncoder(cols=['family','city','state'], m=5.0)
encoder.fit(X_encode, y_encode)
X_train = encoder.transform(X_pretrain)
X_test=encoder.transform(test)

X_train=train_5.drop('sales_log1p', axis=1)
y_train=train_5['sales_log1p']
X_test=test

X_train.head()

y_train.head()

X_test.head()

X_train.isna().sum()

X_test.isna().sum()

X_train.drop(['id','date'], axis=1, inplace=True)
X_test.drop(['id','date'], axis=1, inplace=True)

X_train['WeekOfYear'].dtype

X_train['WeekOfYear'].isna().sum()

X_train['WeekOfYear']=X_train['WeekOfYear'].astype('float')

"""# Machine Learning _ lightgbm"""

import lightgbm as lgbm
from sklearn.model_selection import KFold

params = {
    "objective": "regression",
    "boosting": "gbdt",
    "num_leaves": 1280,
    "learning_rate": 0.05,
    "feature_fraction": 0.85,
    "reg_lambda": 2,
    "metric": "rmse",
}

kf=KFold(n_splits=5)

models=[]

for train_idx, test_idx in kf.split(X_train):
    kf_X_train=X_train.iloc[train_idx]
    kf_y_train=y_train.iloc[train_idx]
    
    kf_X_test=X_train.iloc[test_idx]
    kf_y_test=y_train.iloc[test_idx]
    
    d_training=lgbm.Dataset(kf_X_train,label=kf_y_train,free_raw_data=False)
    d_test=lgbm.Dataset(kf_X_test,label=kf_y_test,free_raw_data=False)
    
    model=lgbm.train(params, train_set=d_training, num_boost_round=1000, valid_sets=[d_training,d_test], 
                     verbose_eval=25, early_stopping_rounds=50)
    models.append(model)

for model in models:
    lgbm.plot_importance(model)
    plt.show()

X_test['WeekOfYear']=X_test['WeekOfYear'].astype('float')

y_pred=np.zeros((X_test.shape[0], len(models)))

for i, model in enumerate(models):
    y_pred[:, i]=np.expm1(model.predict(X_test))

y_pred=np.clip(y_pred,0,None)

y_pred=np.mean(y_pred, axis=1)

y_pred

y_pred.min()

np.save('y_pred_lgbm', y_pred)

"""## *RMSLE Calculation*"""

y_train_pred=np.zeros((X_train.shape[0], len(models)))

for i,model in enumerate(models):
    y_train_pred[:, i]=model.predict(X_train)

y_train_pred=np.clip(y_train_pred, 0, None)

y_train_pred=np.mean(y_train_pred, axis=1)

from sklearn.metrics import mean_squared_error as mse
def RMSLE(y_pred, y_test):
    return np.sqrt(mse(y_test,y_pred))

RMSLE(y_train_pred, y_train)

"""**Preparing Submission**"""

y_pred=y_pred.reshape(-1,1)

submission['pred']=y_pred

submission.head()

submission.drop('sales', axis=1, inplace=True)

submission=submission.rename({'pred':'sales'}, axis=1)

submission.to_csv('submission.csv', index=False, float_format='%.3f')